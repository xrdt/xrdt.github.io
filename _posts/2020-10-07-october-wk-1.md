---
layout: posts
title: "October Wk 1 -- Beginning of Autumn"
---
*Author: Bianca Yang*<br>
*Email: <a href="mailto:ipacifics@gmail.com?subject=Hello from the XDRT Blog">ipacifics@gmail.com</a>*<br>

__Better Engineering Practices__:
* [Handmade Software](https://handmade.network/manifesto)
  * A way to fight against [Wirth's Law](https://en.wikipedia.org/wiki/Wirth%27s_law)
  by taking into account the realities of the hardware we work with and
  building software that actually uses that raw metal speed we've been so
  graciously given by chip designers.
  * [Handmade Hero](https://hero.handmade.network/) is one of their projects.
  * [Smart Serialization](https://yave.handmade.network/blogs/p/2723-how_media_molecule_does_serialization)
    * This is a dead simple, super flexible, super readable, backward
      compatible serialization method used by Media Molecule, which produced
      the Little Big Planet series and Dreams. It's called LBp for Little Big
      Planet and was used for all three releases.
    * Considerations for building a serialization method:
      * Backwards compatibility (open file_v1.dat with program_v3.exe)
      * Forwards compatibility (open file_v3.dat with program_v1.exe)
      * Self-description (tools can parse the data without special knowledge)
      * File size
      * Serialization speed
      * Reliability
      * Flexibility
      * Human readability
      * Platform-independence
      * Complexity for users
      * Degree of abstraction
      * Standards compliance
    * Limitations of the LBP model:
      * Strict, linear revision number maintenance.
        * Probably doesn't scale past 20 people due to conflicts in maintaining
          the revision numbers.
      * No forward compatability.
      * Need to compile C code (the fact that everything is raw C/++ code is
        also what gives this serialization method so much flexibility) to
        understand the binary format. You could have a converted between the
        binary and json if you really want.
    * The [original
   correspondence](https://gist.github.com/OswaldHurlem/4810ad510669097db872c6de305c9df0)
      that spawned the article. The formatting is garbage, but the ideas are
      top notch.
* [Resilience
  Engineering](http://www.kitchensoap.com/2011/04/07/resilience-engineering-part-i/)
  (for web engineering)
  * This article is quite weak on the writing and content density. I pulled out
  perhaps the one useful quote below:
  > Since resilience is about being able to function, rather than being
  > impervious to failure, there is no conflict between productivity and
  > safety.
  * [Efficiency Thoroughness
   Tradeoff](https://erikhollnagel.com/ideas/etto-principle/)
* Paul Graham on [Average Programming
  Languages](http://www.paulgraham.com/avg.html)
  * Programming Languages are habits of mind or tools for thought. Lisp the
    most powerful tool we have. Having better tools is a serious competitive
    advantage.

__Software Architecture__:
* [Base ref to a bunch of links about StackOverflow's
  architecture](http://highscalability.com/stack-overflow-architecture)
* [Scaling Erlang at WhatsApp](https://news.ycombinator.com/item?id=24443128)
  * [Interview with Jan Koum](https://genius.com/Jim-goetz-jim-goetz-and-jan-koum-at-startup-school-sv-2014-annotated)
  * [What can you do to improve infrastructure of a growing software team?](https://news.ycombinator.com/item?id=24443479)
    * [Automated everything](https://news.ycombinator.com/item?id=24445711)
    > as you scale (50 people to 200+), people adapt to depend on processes
    > rather than individuals. If the process is automated and well documented,
    > you'll be have even more power to tweak the inner workings without
    > retraining anyone.
  *
* [What Satoshi Did Not Know](https://www.ifca.ai/pub/fc15/89750001.pdf)
  * People are trolls (penny flooding)
  * Transaction Validity vs Meaning and how to control meaning leakage.
  * Scaling
  * Has [bitcoin failed](https://blog.plan99.net/the-resolution-of-the-bitcoin-experiment-dabb30201f7?gi=854b864d779c)?

__Software Failures__:
* [Preprint of How Hadoop Cluster
  Break](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.296.341&rep=rep1&type=pdf)
  (12 pages) hat tip to Dan Luu for this link.
  * Misconfiguration, especially of memory management caused ~33% of failures.
    * This can be prevented with better configuration checks.
      * There can also be better linkages between different components of a
        configuration file so that users can constrained to move along
        predefined surfaces in the theoretical configuration space.
    * This can also be prevented with better language level (Java) memory
      management tools (lessen mental burden on developers).
    * Better logging, always helps, especially if one can separate the wheat
      from the chaff. Remember that Abhishek said he had a colleague who would
      verify log correctness.
    * Of course, better testing, especially under load, will help confirm the
      effectiveness of any given configuration change. Slow rollouts is another
      deployment strategy that may reduce the severity and frequency of cluster
      failures.
  * Bugs were maybe another 28% of failures.
  * There are lots of nice graphs in this very short paper. I recommend opening
    the original link to study them.
* [Why Do Computers Stop and What Can Be Done About
  It?](https://www.hpl.hp.com/techreports/tandem/TR-85.7.pdf)
  * 1985 paper on techniques used to guarantee fault tolerant (high
    availability) hardware and software systems for the Tandem NonStop Computer
    System.
    * 90 minute outage (time for full cycle of detecting problem, snapshotting
      the system nad restarting everything) in a system with 2 week mean time
      between failures (MTBF)  is 99.6% availability. But 90 minutes without
      access is absolutely unacceptable for critical operations like patient
      monitoring systems in a hospital.
    * Two key metrics: availability and reliability (proportional to MTBF).
      Availability = MTBF / (MTBF + MTTR). MTTR = Mean Time to Recovery.
    * Two key concepts for engineering high availability systems: modularity
      and redundancy.
      * Note that having one without the other is rather meaningless. Von
        Neumann's initial analysis of how to get 100yr MTBF required 20,000
        redundancy. This was because his system lacked modularity; any failure
        would take down the entire system. Contrast this with modern
        electronics systems, which only require a redundancy of 2 because they
        have excellent modularity.
      * Another example. In 1985, discs were rated for 10,000 hrs MTBF (~ 1yr).
        If you duplex the discs and assume complete independence of disc
        failures, the MTBF extends to 1000 years+.
    * Engineering Hardware Availability:
      * Modularize.
      * Each module should have MTBF 1yr+.
      * Each module should fail fast -- either it does the right thing or
        stops.
      * Set up failure detectors for each module.
      * Have redundant modules that can take over when its partner fails. This
        gives an apparent MTTR of seconds.
  * Why does software fail:

  | Maintenance | 25% |
  | Operations | 9% |
  | Configuration | 8% |
  | Total Administrative Failures | 42% |
  | {} |
  | Software | 25% |
  | {} |
  | Communications Controllers | 6% |
  | Disc | 7% |
  | Total Hardware | 18% |
  | {} |
  | Software | 25% |
  | Environment (power, communications, facilities) | 14% |

  * The table shows that the keys to success in engineering high software
    availability are: tolerating operations and software faults.
  * Here's another two tidbits to color your understanding of the table. (1)
    New and changing systems have higher failure rates. (2) Many outages were
    caused by known bugs. Sounds contradictory, until you differentiate between
    hardware and software maintenance:
    * For hardware, it's best to install fixes as soon as possible.
    * For software, it's best to wait until the software has been validated in
      the field for some time.
  * Now how do you Engineer Software Availability:
    * Modularize the software with run-time checks with processes as the unit
      of containment.
    * Make sure software modules are fail fast. Use defensive programming:
      check inputs, outputs, data structures to constantly validate correctness
      of operations.
    * Use process-pairs.
      * Lockstep pairs: Primary and backup processes execute synchronously. The
        backup takes over once the primary fails. It's easy to implement but
        both fail in the same way and won't allow for recovery from Heisenbugs
        (issues that can recover from reexecution).
      * State checkpointing: Gives great performance but hard to implementing
        good checkpointing (this might have changed with Kafka).
      * Autmoatic checkpointing: The kernel does checkpointing. Can have high
        execution cost due to size of checkpointing messages.
      * Delta Checkpointing: Logical rather than physical updates are sent. Can
        reduce message traffic and size of messages. Also hard to program.
      * Persistent Checkpointing: Backup takes over from primary process in an
        amnesic state. Issue is that the database and other services may have
        been corrupted and that state is lost. These are issues that can
        readily resolved with transactions, so this is the recommended method.
    * Transactions preserve data integrity (reliability).
      * Must be A (atomic) C (consistent) I (have integrity) D (durable)
      * Transactions can undo incomplete state, thus allowing persistent
        checkpoints to restart on a clean slate.
    * Tidbit on communication lines: open sockets and use transaction ids to
      keep track of state of messages.
    * Tidbit on storage: local replication is good, but remote replication is
      even better because of the diversity of management and hardware and
      environment provided by the remote location. Transactions can ensure that
      data is in a consistent state. Geographic isolation of data is also an
      effective technique.
  * There are no suggestions on improving operations. People are already doing
    the best they can. We should try to reduce the surface area of human
    interaction.
* [Crash-Only Software](https://lwn.net/Articles/191059/)
  * Definition: software that crashes safely and recovers quickly. You can only
    stop it with a crash. You can only start it in recovery.
  * It's no free lunch relative to software that supports normal shutdown and
    start up.
    * You need excellent checkpointing and recovery code to make it
    work.
    * You also can't just force restart the whole system any time something
      malfunctions. Microreboots, of threads, for example, are an attractive
      and effective strategy.
    * How do you implement a crash mechanism that is independent of your system?
      * How do you guarantee the crash will happen when you want it to happen
        and not get blocked by other requests in a single-threaded event loop
        or by the need to log a final shut down statement onto an already full
        disk?
    * How do you trade off performance vs. recovery time vs. reliability?
      * You can't be checkpointing every millisecond. You also can't checkpoint
        every time the user shuts a system down (when was the last time you
        shut down your browser, for example?).
      * Unless you have some magical way of knowing that your software is
        absolutely never ever going to crash, it might turn out that the
        savings from having fast crash-recovery outweigh the drags on runtime
        performance.
    * You might not want to roll your own state storing system... Lots of
      things can go wrong here.
    * Consider how to design crash-safety in between components (like threads).
  * If you think this idea is quackery, you can go back to using a word
    processor that doesn't have auto-save on a system that crashes at a random
    time every hour.
  > Properly implemented, crash-only software produces higher quality, more
  > reliable code; poorly understood it results in lazy programming.
  * Of course [Erlang](http://erlang.org/download/getting_started-5.4.pdf)
  [embodies this principle
  beautifully](http://erlang.org/download/armstrong_thesis_2003.pdf).
* [Usenix paper on Simple Testing to Prevent Critical Failures in Distributed
  Systemk](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf).
  * Fantastic paper that has code to help alleviate the problem and plenty of
    concrete examples to shock you into cleaning up those pesky error handlers
    in your code base.
  * The approach here was to understand how a sequence of errors could lead to
    component failures. Tested Cassandra, HBase, HDFS, MapReduce, and Redis.
  > almost all (92%) of the catastrophic system failures are the result of
  > incorrect handling of non-fatal errors explicitly signaled in software
  <br>---<br>
  > in 35% of the catastrophic failures, the faults in the error handling code
  > fall into three trivial patterns: (i) the error handler is simply empty or
  > only contains a log printing statement (25%), (ii) the error handler aborts
  > the cluster on an overly-general exception (8%), and (iii) the error
  > handler contains expressions like “FIXME” or “TODO” in the comments (2%)
  * 23% of failures were caused by system specific failures to handle errors
    properly. Reverse engineering test cases to cover these messages may be the
    best strategy for dealing with these failures.
  * 33% of failures were from complex bugs in error handling logic.
  <br>---<br>
  > in 58% of the catastrophic failures, the underlying faults could easily
  > have been detected through simple testing of error handling code.
  * The logs are helpful in that they do often contain the reason for failure.
    The issue that remains is the noisiness of the logs:
    > in 76% of the failures, the system emits explicit failure messages; and
    > in 84% of the failures, all of the triggering events that caused the
    > failure are printed into the log before failing
  * Also, most of the failures this team analyzed could be reproduced with unit
    tests.
    * 77% of failures require multiple input events (2+).
      * 88% of those failures are sequence specific.
  * Top 5 input events by % of times these events are required to trigger
    failures:

    | Starting a Service | 58% |
    | File/database write from client | 32% |
    | Unreachable node (network crash, node error) | 24% |
    | Configuration change | 23% |
    | Adding a node to running system | 15% |

  * 98% of failures manifest on systems with 3 nodes. 74% of failures are
    deterministic. Among non-deterministic failures, 53% have timing
    constraints only on input events (can be simulated by programmer).  77% of
    production failures can be reproduced by a unit test (Redis is the low
    outlier with only 58% reproducible. This is because Redis has a limited
    unit testing framework. Interactions limited to the command line.).
    * The hard to repro non-deterministic failures were mainly caused by thread
      interleavings.
  * [Aspirator Static Checker](https://github.com/diy1/aspirator) that checks
    for ignored or overcaught or unimplemented exception handlers.
  * Symbolic execution can also help reconstruct code pathways to error
    handling code blocks.
* [Never hurts to test the tests...](https://wiki.c2.com/?TracerBullets)
* [Why Erlang is Beautiful](https://t.co/94pZAHpJxp?amp=1)
* [How Rust and Elm Make Writing Code That Much Better](https://t.co/NSPV9lRcIO?amp=1)
  * If it compiles, it probably works. Vanier said that about OCaml, too.

__Personal Connections__:
* [Stephanie Hurlburt graciously compiled a list of engineering
  mentors](http://stephaniehurlburt.com/blog/2016/11/14/list-of-engineers-willing-to-mentor-you)
* A Story about [How Learning Scheme Opened Up One Computer Enthusiast's Mind
  to What Interfacing with Computers Is
  About](http://www.trollope.org/scheme.html)
  * Stifling structure of the AP Computer Programming Class:
  >  It seemed that my classmates' programming strategy was to let the computer
  >  find their errors in the hope that this would somehow help them solve the
  >  problem. It never occurred to them that the computer is simply a testing
  >  ground for a well-thought-out idea. [...] For most of my classmates, this
  >  was their fourth year in the computer education system, and all it had
  >  taught them was to rely too much on the computer and not enough on
  >  themselves.
  * Why do people come to rely on the computer instead of their heads?
  > [W]hat is currently thought to be important for students to learn in
  > programming courses, namely, syntax. [...] As a student who has been
  > through the system, who has had to waste the majority of my patience,
  > concentration, and effort on keeping the syntax of my programs straight,
  > leaving barely enough of these qualities to devote to solving the problem,
  > I can tell you that such a belief could not be more wrong. This method of
  > teaching is brainwashing. It is not like brainwashing or similar to
  > brainwashing; it is brainwashing. It is a danger to computer science,
  > sending out trained hackers instead of enthusiastic visionaries.
  * Scheme became his gateway into understanding fundamental computer science
    concepts.
  > These things occur so naturally in Scheme that I couldn't help but
  > understand. After mastering the concept, I could then go back into my
  > Pascal class and master the code.
  * Where Scheme differs from Pascal:
  > The strange thing is that Pascal sets you up to fall into the trap of
  > complexity and to disobey the laws of top-down design. As a procedural
  > language, it enticed us into trying to do too much in one procedure simply
  > because it could be done. With Scheme, I couldn't write a function that had
  > more than one purpose. It is as inherently top-down as it is inherently
  > recursive.
  * And the closing thought which I wholeheartedly agree with:
  > Kids never liked rules anyway, and that's all we are---kids.
  * There is some essence of people that stays constant their whole lives. The
  intuition and curiosity that kids are born with is a powerful and rich medium
  which the adult intellect is merely a sharpening and refinement of. It's
  foolish to ignore children because sometimes they say or do the obvious
  things that we've spent our whole lives suppressing our instinct to publicly
  reveal.

__Entertainment__:
* After a [several months hiatus](../../06/30/june-links.html), I *finally*
  picked Diamond Is Unbreakable back up and finished it. I got fed up with the
  lack of plot movement and the effort I was expending to read it in Chinese,
  so I went on to read greater things (haha... what could be great than this
  series?). Anyway, the final battle is ridiculously epic. It's worth all the
  intermediate steps, even the random interludes with the alien. It's time now
  to jump to the beginning of the Jojo series and continue my journey from
  there.

__Daily Life__:
* [RSS IS NOT DEAD](https://news.ycombinator.com/item?id=24658424)
  * I've been using Reeder, but I like keeping tabs open for further
    inspiration on what it takes to build a beautiful product.
* [Heat the pan then add
  oil](https://cooking.stackexchange.com/questions/2690/do-you-heat-the-pan-first-then-add-oil-or-put-the-oil-in-and-heat-up-with-the)
  or [heat the oil with the
  pan](https://cooking.stackexchange.com/questions/99543/why-do-you-need-to-heat-the-pan-before-heating-the-olive-oil/99544)?
* Slowing expanding my mind. One day, I found [Lexical Closures](https://wiki.c2.com/?LexicalClosure)
* Fri, Oct 2 was my last day with ThoughtSpot. I gathered the phone numbers
  of all the people I wanted to have in my Rolodex and shared my email /
  phone with everybody I thought would want to have me in their Rolodex. I got
  locked out of everything around 4pm. I had a great farewell party with
  everyone at 1:30pm (Abhinav, Wei, Shannon, Ben, Nikhil, Sudarshan, Rahul,
  Archit, Aaron, Ajay, Sanchit, Rakesh).
  * I'm nervous about moving to BetterOmics. I'm not convinced it will be my
    final resting place because I don't think it will provide me enough
    exposure to non-software forms of engineering and because I don't know if
    I ultimately care that much about software engineering (esp. enterprise).
    But I hope (I'm hoping so hard) I'll have as good of (and maybe greater)
    of a run with BetterOmics as I had with ThoughtSpot (things really ramped
    up to a head in my final month with ThoughtSpot. I really left on a good
    note) so I can launch from BetterOmics onto something that really will
    light a fire under my butt and keep me running for years.
